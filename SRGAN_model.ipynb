{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***MODEL***"
      ],
      "metadata": {
        "id": "Qr3iCHaADnmB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "p8uiapSFybfV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  #Conv, batcnorm, then p or leaky relu\n",
        "  def __init__(self, in_channels, out_channels, discriminator=False,use_act=True,use_bn=True,**kwargs):\n",
        "    super().__init__()\n",
        "    self.use_act = use_act\n",
        "    self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n",
        "    self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "    self.act = (\n",
        "        nn.LeakyReLU(0.2, inplace=True)\n",
        "        if discriminator\n",
        "        else nn.PReLU(num_parameters = out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))"
      ],
      "metadata": {
        "id": "CFry1bZIyrle"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "  def __init__(self, in_c, scale_factor):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_c, in_c*scale_factor**2, 3, 1, 1)\n",
        "    self.ps = nn.PixelShuffle(scale_factor)#in_c*4 H,, W -- >> in_c, H*2, W*2\n",
        "    self.act = nn.PReLU(num_parameters=in_c)\n",
        "\n",
        "  def forward(self, x ):\n",
        "    return self.act(self.ps(self.conv(x)))"
      ],
      "metadata": {
        "id": "e1JRBrbPy13k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, in_channels):\n",
        "    super().__init__()\n",
        "    self.block1 = ConvBlock(\n",
        "        in_channels,\n",
        "        in_channels,\n",
        "        kernel_size = 3,\n",
        "        stride = 1,\n",
        "        padding = 1\n",
        "    )\n",
        "\n",
        "    self.block2 = ConvBlock(\n",
        "        in_channels,\n",
        "        in_channels,\n",
        "        kernel_size = 3,\n",
        "        stride = 1,\n",
        "        padding = 1,\n",
        "        use_act = False\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.block1(x)\n",
        "    out = self.block2(out)\n",
        "    return out + x"
      ],
      "metadata": {
        "id": "O3R6Ew8Dy5I0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_channels = 64, num_blocks=16):\n",
        "    super().__init__()\n",
        "    self.initial = ConvBlock(in_channels, num_channels, kernel_size = 9, stride = 1, padding = 4, use_bn = False)\n",
        "    self.residuals = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_blocks)])\n",
        "    self.convblock = ConvBlock(num_channels, num_channels, kernel_size=3, stride=1, padding=1, use_act=False)\n",
        "    self.upsamples = nn.Sequential(UpsampleBlock(num_channels, scale_factor = 2), UpsampleBlock(num_channels, scale_factor=2))\n",
        "    self.final = nn.Conv2d(num_channels, in_channels, kernel_size=9, stride = 1, padding = 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    initial = self.initial(x)\n",
        "    x = self.residuals(initial)\n",
        "    x = self.convblock(x) + initial\n",
        "    x = self.upsamples(x)\n",
        "    return torch.tanh(self.final(x))"
      ],
      "metadata": {
        "id": "TqOxTZ8Xy71W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels=3, features=[64,64,128,128,256,256,512,512]):\n",
        "    super().__init__()\n",
        "    blocks = []\n",
        "    for idx, feature in enumerate(features):\n",
        "      blocks.append(\n",
        "          ConvBlock(\n",
        "              in_channels,\n",
        "              feature,\n",
        "              kernel_size=3,\n",
        "              stride=1+idx%2,\n",
        "              padding = 1,\n",
        "              discriminator = True,\n",
        "              use_act = True,\n",
        "              use_bn=False if idx == 0 else True\n",
        "          )\n",
        "      )\n",
        "      in_channels = feature\n",
        "\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((6,6)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512*6*6, 1024),\n",
        "        nn.LeakyReLU(0.2,inplace=True),\n",
        "        nn.Linear(1024,1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.blocks(x)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "JU4q_gJFzaHO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  low_resolution = 24\n",
        "  with torch.cuda.amp.autocast():\n",
        "    x = torch.randn((5,3,low_resolution,low_resolution))\n",
        "    gen = Generator()\n",
        "    gen_out = gen(x)\n",
        "    disc = Discriminator()\n",
        "    disc_out = disc(x)\n",
        "    \n",
        "    print(gen_out.shape)\n",
        "    print(disc_out.shape)"
      ],
      "metadata": {
        "id": "jtBqxGCWzcUo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iium07j8AmPj",
        "outputId": "c233368f-cf27-42da-e6ea-8ff6ffcb7f48"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3, 96, 96])\n",
            "torch.Size([5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VGG LOSS FUNCTION***"
      ],
      "metadata": {
        "id": "cFR8kWK6Dhnr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciKOkbNUDusm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}